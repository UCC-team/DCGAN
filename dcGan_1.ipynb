{"cells":[{"outputs":[{"output_type":"stream","text":"time: 624 µs\n","name":"stdout"}],"execution_count":3,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"D0BF21F36C39412BB133B5447CC02444","scrolled":false,"hide_input":false}},{"outputs":[],"execution_count":1,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"97729974AD3B41CB96E8DD28AAB80174","scrolled":false}},{"metadata":{"id":"3F3CD177CC074A959607D681EAA2EEBC","hide_input":false,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","source":"# -*- coding:utf-8 -*-\r\n'''\r\nDCGAN on MNIST using Keras\r\nAuthor: YanZY\r\nProject: https://github.com/UCC-team/DCGAN/\r\nDependencies: tensorflow 1.0 and keras 2.0\r\nUsage: python3 dcgan_mnist.py\r\n'''\r\n\r\nimport numpy as np\r\nimport time\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Activation, Flatten, Reshape\r\nfrom keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\r\nfrom keras.layers import LeakyReLU, Dropout\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.optimizers import Adam, RMSprop\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nclass ElapsedTimer(object):\r\n    def __init__(self):\r\n        self.start_time = time.time()\r\n    def elapsed(self,sec):\r\n        if sec < 60:\r\n            return str(sec) + \" sec\"\r\n        elif sec < (60 * 60):\r\n            return str(sec / 60) + \" min\"\r\n        else:\r\n            return str(sec / (60 * 60)) + \" hr\"\r\n    def elapsed_time(self):\r\n        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\r\n\r\nclass DCGAN(object):\r\n    def __init__(self, img_rows=28, img_cols=28, channel=1):\r\n\r\n        self.img_rows = img_rows\r\n        self.img_cols = img_cols\r\n        self.channel = channel\r\n        self.D = None   # discriminator\r\n        self.G = None   # generator\r\n        self.AM = None  # adversarial model\r\n        self.DM = None  # discriminator model\r\n\r\n    # (W−F+2P)/S+1\r\n    def discriminator(self):\r\n        if self.D:\r\n            return self.D\r\n        self.D = Sequential()\r\n        depth = 64\r\n        dropout = 0.4\r\n        # In: 28 x 28 x 1, depth = 1\r\n        # Out: 10 x 10 x 1, depth=64\r\n        input_shape = (self.img_rows, self.img_cols, self.channel)\r\n        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\r\n            padding='same', activation=LeakyReLU(alpha=0.2)))\r\n        self.D.add(Dropout(dropout))\r\n\r\n        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same',\\\r\n                activation=LeakyReLU(alpha=0.2)))\r\n        self.D.add(Dropout(dropout))\r\n\r\n        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same',\\\r\n                activation=LeakyReLU(alpha=0.2)))\r\n        self.D.add(Dropout(dropout))\r\n\r\n        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same',\\\r\n                activation=LeakyReLU(alpha=0.2)))\r\n        self.D.add(Dropout(dropout))\r\n\r\n        # Out: 1-dim probability\r\n        self.D.add(Flatten())\r\n        self.D.add(Dense(1))\r\n        self.D.add(Activation('sigmoid'))\r\n        self.D.summary()\r\n        return self.D\r\n\r\n    def generator(self):\r\n        if self.G:\r\n            return self.G\r\n        self.G = Sequential()\r\n        dropout = 0.4\r\n        depth = 64+64+64+64\r\n        dim = 7\r\n        # In: 100\r\n        # Out: dim x dim x depth\r\n        self.G.add(Dense(dim*dim*depth, input_dim=100))\r\n        self.G.add(BatchNormalization(momentum=0.9))\r\n        self.G.add(Activation('relu'))\r\n        self.G.add(Reshape((dim, dim, depth)))\r\n        self.G.add(Dropout(dropout))\r\n\r\n        # In: dim x dim x depth\r\n        # Out: 2*dim x 2*dim x depth/2\r\n        self.G.add(UpSampling2D())\r\n        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\r\n        self.G.add(BatchNormalization(momentum=0.9))\r\n        self.G.add(Activation('relu'))\r\n\r\n        self.G.add(UpSampling2D())\r\n        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\r\n        self.G.add(BatchNormalization(momentum=0.9))\r\n        self.G.add(Activation('relu'))\r\n\r\n        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\r\n        self.G.add(BatchNormalization(momentum=0.9))\r\n        self.G.add(Activation('relu'))\r\n\r\n        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\r\n        self.G.add(Conv2DTranspose(1, 5, padding='same'))\r\n        self.G.add(Activation('sigmoid'))\r\n        self.G.summary()\r\n        return self.G\r\n\r\n    def discriminator_model(self):\r\n        if self.DM:\r\n            return self.DM\r\n        optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\r\n        self.DM = Sequential()\r\n        self.DM.add(self.discriminator())\r\n        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\r\n            metrics=['accuracy'])\r\n        return self.DM\r\n\r\n    def adversarial_model(self):\r\n        if self.AM:\r\n            return self.AM\r\n        optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\r\n        self.AM = Sequential()\r\n        self.AM.add(self.generator())\r\n        self.AM.add(self.discriminator())\r\n        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\r\n            metrics=['accuracy'])\r\n        return self.AM\r\n\r\nclass MNIST_DCGAN(object):\r\n    def __init__(self):\r\n        self.img_rows = 28\r\n        self.img_cols = 28\r\n        self.channel = 1\r\n\r\n        self.x_train = input_data.read_data_sets(\"mnist\",\\\r\n                one_hot=True).train.images\r\n        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\r\n                self.img_cols, 1).astype(np.float32)\r\n\r\n        self.DCGAN = DCGAN()\r\n        self.discriminator =  self.DCGAN.discriminator_model()\r\n        self.adversarial = self.DCGAN.adversarial_model()\r\n        self.generator = self.DCGAN.generator()\r\n\r\n    def train(self, train_steps=2000, batch_size=256, save_interval=0):\r\n        noise_input = None\r\n        if save_interval>0:\r\n            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\r\n        for i in range(train_steps):\r\n            images_train = self.x_train[np.random.randint(0,\r\n                self.x_train.shape[0], size=batch_size), :, :, :]\r\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\r\n            images_fake = self.generator.predict(noise)\r\n            x = np.concatenate((images_train, images_fake))\r\n            y = np.ones([2*batch_size, 1])\r\n            y[batch_size:, :] = 0\r\n            d_loss = self.discriminator.train_on_batch(x, y)\r\n\r\n            y = np.ones([batch_size, 1])\r\n            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\r\n            a_loss = self.adversarial.train_on_batch(noise, y)\r\n            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\r\n            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\r\n            print(log_mesg)\r\n            if save_interval>0:\r\n                if (i+1)%save_interval==0:\r\n                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\r\n                        noise=noise_input, step=(i+1))\r\n\r\n    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\r\n        filename = 'mnist.png'\r\n        if fake:\r\n            if noise is None:\r\n                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\r\n            else:\r\n                filename = \"mnist_%d.png\" % step\r\n            images = self.generator.predict(noise)\r\n        else:\r\n            i = np.random.randint(0, self.x_train.shape[0], samples)\r\n            images = self.x_train[i, :, :, :]\r\n\r\n        plt.figure(figsize=(10,10))\r\n        for i in range(images.shape[0]):\r\n            plt.subplot(4, 4, i+1)\r\n            image = images[i, :, :, :]\r\n            image = np.reshape(image, [self.img_rows, self.img_cols])\r\n            plt.imshow(image, cmap='gray')\r\n            plt.axis('off')\r\n        plt.tight_layout()\r\n        if save2file:\r\n            plt.savefig(filename)\r\n            plt.close('all')\r\n        else:\r\n            plt.show()\r\n\r\nif __name__ == '__main__':\r\n    mnist_dcgan = MNIST_DCGAN()\r\n    timer = ElapsedTimer()\r\n    mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\r\n    timer.elapsed_time()\r\n    mnist_dcgan.plot_images(fake=False, save2file=True)","outputs":[{"output_type":"stream","text":"Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nWARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting mnist/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nWARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting mnist/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting mnist/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting mnist/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.5/site-packages/keras/activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n  identifier=identifier.__class__.__name__))\n","name":"stderr"},{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 7, 7, 128)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 4, 4, 256)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 4, 4, 512)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 8193      \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 4,311,553\nTrainable params: 4,311,553\nNon-trainable params: 0\n_________________________________________________________________\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 12544)             1266944   \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 12544)             50176     \n_________________________________________________________________\nactivation_2 (Activation)    (None, 12544)             0         \n_________________________________________________________________\nreshape_1 (Reshape)          (None, 7, 7, 256)         0         \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 7, 7, 256)         0         \n_________________________________________________________________\nup_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n_________________________________________________________________\nconv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 14, 14, 128)       512       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 14, 14, 128)       0         \n_________________________________________________________________\nup_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 28, 28, 64)        256       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 28, 28, 64)        0         \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 28, 28, 32)        128       \n_________________________________________________________________\nactivation_5 (Activation)    (None, 28, 28, 32)        0         \n_________________________________________________________________\nconv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 28, 28, 1)         0         \n=================================================================\nTotal params: 2,394,241\nTrainable params: 2,368,705\nNon-trainable params: 25,536\n_________________________________________________________________\nWARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n0: [D loss: 0.691934, acc: 0.515625]  [A loss: 16.118101, acc: 0.000000]\n1: [D loss: 7.951656, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n2: [D loss: 7.935910, acc: 0.500000]  [A loss: 16.118101, acc: 0.000000]\n3: [D loss: 6.001930, acc: 0.519531]  [A loss: 0.000000, acc: 1.000000]\n4: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n5: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n6: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n7: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n8: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n9: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n10: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n11: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n12: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n13: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n14: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n15: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n16: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n17: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n18: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n19: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n20: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n21: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n22: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n23: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n24: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n25: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n26: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n27: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n28: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n29: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n30: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n31: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n32: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n33: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n34: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n35: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n36: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n37: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n38: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n39: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n40: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n41: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n42: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n43: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n44: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n45: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n46: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n47: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n48: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n49: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n50: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n51: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n52: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n53: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n54: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n55: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n56: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n57: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n58: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n59: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n60: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n61: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n62: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n63: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n64: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n65: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n66: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n67: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n68: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n69: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n70: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n71: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n72: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n73: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n74: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n75: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n76: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n77: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n78: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n79: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n80: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n81: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n82: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n83: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n84: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n85: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n86: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n87: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n88: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n89: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n90: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n91: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n92: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n93: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n94: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n95: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n96: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n97: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n98: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n99: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n100: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n101: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n102: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n103: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n104: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n105: [D loss: 7.973256, acc: 0.498047]  [A loss: 0.000000, acc: 1.000000]\n106: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n107: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n108: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n109: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n110: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n111: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n112: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n113: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n114: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n115: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n116: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n117: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n118: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n119: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n120: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n121: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n122: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n123: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n124: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n125: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n126: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n127: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n128: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n129: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n130: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n131: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n132: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n133: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n134: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n135: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n136: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n137: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n138: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n139: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n140: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n141: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n142: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n143: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n144: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n145: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n146: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n147: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n148: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n149: [D loss: 7.971192, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n","name":"stdout"}],"execution_count":1},{"metadata":{"id":"8BEE8127FD22414EBE074E81FB84C22C"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.5.5","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":0}